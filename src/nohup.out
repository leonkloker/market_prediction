Traceback (most recent call last):
  File "train.py", line 12, in <module>
    from data import *
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/data.py", line 3, in <module>
    import pandas as pd
ModuleNotFoundError: No module named 'pandas'
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1694, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1185, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 856, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 591, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1932, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 89, in <module>
    torch.save(model.state_dict(), 
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/serialization.py", line 440, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/serialization.py", line 315, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/serialization.py", line 288, in __init__
    super().__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory ../outputs/models does not exist.
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1694, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1185, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1932, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 591, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 856, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 659, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 328, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 112, in <module>
    out = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 53, in forward
    src = self.embedding(src)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/utils.py", line 40, in forward
    return x + self.pos_encoding[:x.size(-2), :]
RuntimeError: The size of tensor a (10733) must match the size of tensor b (10000) at non-singleton dimension 1
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 591, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 857, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1933, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1185, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1694, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 329, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 476, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1074, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 941, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 659, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 8, in <module>
    from ignite.metrics import Accuracy
ModuleNotFoundError: No module named 'ignite.metrics'
Traceback (most recent call last):
  File "train.py", line 8, in <module>
    from ignite.metrics import Accuracy
ModuleNotFoundError: No module named 'ignite.metrics'
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Sigmoid: 1-5                                     [1, 200, 1]               --
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 85, in <module>
    val_loss += criterion(out[:,int(x.size(-2)*TRAIN_INTERVAL[1]):,:], y[:,int(x.size(-2)*TRAIN_INTERVAL[1]),:])
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 619, in forward
    return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 3089, in binary_cross_entropy
    raise ValueError(
ValueError: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1186, 1])) is deprecated. Please ensure they have the same size.
