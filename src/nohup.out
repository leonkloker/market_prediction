/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              2,368
├─Time2Vec: 1-2                                    [1, 200, 72]              --
│    └─Linear: 2-1                                 [200, 8]                  16
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 72]              (recursive)
│    └─Linear: 2-2                                 [200, 8]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 72]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 72]              --
│    │    └─ModuleList: 3-1                        --                        91,992
│    │    └─LayerNorm: 3-2                         [1, 200, 72]              144
│    └─TransformerDecoder: 2-4                     [1, 200, 72]              --
│    │    └─ModuleList: 3-3                        --                        155,496
│    │    └─LayerNorm: 3-4                         [1, 200, 72]              144
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 36]              2,628
│    └─Dropout: 2-6                                [1, 200, 36]              --
│    └─ReLU: 2-7                                   [1, 200, 36]              --
│    └─Linear: 2-8                                 [1, 200, 1]               37
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 252,825
Trainable params: 252,825
Non-trainable params: 0
Total mult-adds (M): 0.04
====================================================================================================
Input size (MB): 0.06
Forward/backward pass size (MB): 2.21
Params size (MB): 0.14
Estimated Total Size (MB): 2.41
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 90, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 65, in forward
    output = self.transformer(src, tgt, tgt_mask=self.tgt_mask, src_mask=self.src_mask, memory_mask=self.memory_mask)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 146, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 716, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 725, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 3; 39.59 GiB total capacity; 35.34 GiB already allocated; 792.19 MiB free; 37.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              2,368
├─Time2Vec: 1-2                                    [1, 200, 72]              --
│    └─Linear: 2-1                                 [200, 8]                  16
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 72]              (recursive)
│    └─Linear: 2-2                                 [200, 8]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 72]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 72]              --
│    │    └─ModuleList: 3-1                        --                        91,992
│    │    └─LayerNorm: 3-2                         [1, 200, 72]              144
│    └─TransformerDecoder: 2-4                     [1, 200, 72]              --
│    │    └─ModuleList: 3-3                        --                        155,496
│    │    └─LayerNorm: 3-4                         [1, 200, 72]              144
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 36]              2,628
│    └─Dropout: 2-6                                [1, 200, 36]              --
│    └─ReLU: 2-7                                   [1, 200, 36]              --
│    └─Linear: 2-8                                 [1, 200, 1]               37
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 252,825
Trainable params: 252,825
Non-trainable params: 0
Total mult-adds (M): 0.04
====================================================================================================
Input size (MB): 0.06
Forward/backward pass size (MB): 2.21
Params size (MB): 0.14
Estimated Total Size (MB): 2.41
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 90, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 65, in forward
    output = self.transformer(src, tgt, tgt_mask=self.tgt_mask, src_mask=self.src_mask, memory_mask=self.memory_mask)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 146, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 716, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 725, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 4; 39.59 GiB total capacity; 35.35 GiB already allocated; 1.81 GiB free; 36.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              2,368
├─Time2Vec: 1-2                                    [1, 200, 72]              --
│    └─Linear: 2-1                                 [200, 8]                  16
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 72]              (recursive)
│    └─Linear: 2-2                                 [200, 8]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 72]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 72]              --
│    │    └─ModuleList: 3-1                        --                        91,992
│    │    └─LayerNorm: 3-2                         [1, 200, 72]              144
│    └─TransformerDecoder: 2-4                     [1, 200, 72]              --
│    │    └─ModuleList: 3-3                        --                        155,496
│    │    └─LayerNorm: 3-4                         [1, 200, 72]              144
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 36]              2,628
│    └─Dropout: 2-6                                [1, 200, 36]              --
│    └─ReLU: 2-7                                   [1, 200, 36]              --
│    └─Linear: 2-8                                 [1, 200, 1]               37
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 252,825
Trainable params: 252,825
Non-trainable params: 0
Total mult-adds (M): 0.04
====================================================================================================
Input size (MB): 0.06
Forward/backward pass size (MB): 2.21
Params size (MB): 0.14
Estimated Total Size (MB): 2.41
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 90, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 65, in forward
    output = self.transformer(src, tgt, tgt_mask=self.tgt_mask, src_mask=self.src_mask, memory_mask=self.memory_mask)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 146, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 716, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 725, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.14 GiB (GPU 4; 39.59 GiB total capacity; 34.73 GiB already allocated; 1.44 GiB free; 36.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              2,368
├─Time2Vec: 1-2                                    [1, 200, 72]              --
│    └─Linear: 2-1                                 [200, 8]                  16
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 72]              (recursive)
│    └─Linear: 2-2                                 [200, 8]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 72]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 72]              --
│    │    └─ModuleList: 3-1                        --                        91,992
│    │    └─LayerNorm: 3-2                         [1, 200, 72]              144
│    └─TransformerDecoder: 2-4                     [1, 200, 72]              --
│    │    └─ModuleList: 3-3                        --                        155,496
│    │    └─LayerNorm: 3-4                         [1, 200, 72]              144
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 36]              2,628
│    └─Dropout: 2-6                                [1, 200, 36]              --
│    └─ReLU: 2-7                                   [1, 200, 36]              --
│    └─Linear: 2-8                                 [1, 200, 1]               37
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 252,825
Trainable params: 252,825
Non-trainable params: 0
Total mult-adds (M): 0.04
====================================================================================================
Input size (MB): 0.06
Forward/backward pass size (MB): 2.21
Params size (MB): 0.14
Estimated Total Size (MB): 2.41
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 90, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 65, in forward
    output = self.transformer(src, tgt, tgt_mask=self.tgt_mask, src_mask=self.src_mask, memory_mask=self.memory_mask)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 146, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 716, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 725, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 4; 39.59 GiB total capacity; 35.36 GiB already allocated; 1.64 GiB free; 36.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              2,368
├─Time2Vec: 1-2                                    [1, 200, 72]              --
│    └─Linear: 2-1                                 [200, 8]                  16
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 72]              (recursive)
│    └─Linear: 2-2                                 [200, 8]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 72]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 72]              --
│    │    └─ModuleList: 3-1                        --                        91,992
│    │    └─LayerNorm: 3-2                         [1, 200, 72]              144
│    └─TransformerDecoder: 2-4                     [1, 200, 72]              --
│    │    └─ModuleList: 3-3                        --                        155,496
│    │    └─LayerNorm: 3-4                         [1, 200, 72]              144
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 36]              2,628
│    └─Dropout: 2-6                                [1, 200, 36]              --
│    └─ReLU: 2-7                                   [1, 200, 36]              --
│    └─Linear: 2-8                                 [1, 200, 1]               37
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 252,825
Trainable params: 252,825
Non-trainable params: 0
Total mult-adds (M): 0.04
====================================================================================================
Input size (MB): 0.06
Forward/backward pass size (MB): 2.21
Params size (MB): 0.14
Estimated Total Size (MB): 2.41
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 90, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 65, in forward
    output = self.transformer(src, tgt, tgt_mask=self.tgt_mask, src_mask=self.src_mask, memory_mask=self.memory_mask)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 146, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 716, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 725, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 7; 39.59 GiB total capacity; 35.36 GiB already allocated; 1.90 GiB free; 36.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              2,368
├─Time2Vec: 1-2                                    [1, 200, 72]              --
│    └─Linear: 2-1                                 [200, 8]                  16
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 72]              (recursive)
│    └─Linear: 2-2                                 [200, 8]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 72]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 72]              --
│    │    └─ModuleList: 3-1                        --                        91,992
│    │    └─LayerNorm: 3-2                         [1, 200, 72]              144
│    └─TransformerDecoder: 2-4                     [1, 200, 72]              --
│    │    └─ModuleList: 3-3                        --                        155,496
│    │    └─LayerNorm: 3-4                         [1, 200, 72]              144
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 36]              2,628
│    └─Dropout: 2-6                                [1, 200, 36]              --
│    └─ReLU: 2-7                                   [1, 200, 36]              --
│    └─Linear: 2-8                                 [1, 200, 1]               37
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 252,825
Trainable params: 252,825
Non-trainable params: 0
Total mult-adds (M): 0.04
====================================================================================================
Input size (MB): 0.06
Forward/backward pass size (MB): 2.21
Params size (MB): 0.14
Estimated Total Size (MB): 2.41
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 90, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 65, in forward
    output = self.transformer(src, tgt, tgt_mask=self.tgt_mask, src_mask=self.src_mask, memory_mask=self.memory_mask)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 146, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 716, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 725, in _sa_block
    x = self.self_attn(x, x, x,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 7; 39.59 GiB total capacity; 35.36 GiB already allocated; 2.05 GiB free; 36.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
