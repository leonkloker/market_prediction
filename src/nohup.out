Traceback (most recent call last):
  File "train.py", line 12, in <module>
    from data import *
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/data.py", line 3, in <module>
    import pandas as pd
ModuleNotFoundError: No module named 'pandas'
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1694, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1185, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 856, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 591, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1932, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 89, in <module>
    torch.save(model.state_dict(), 
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/serialization.py", line 440, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/serialization.py", line 315, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/serialization.py", line 288, in __init__
    super().__init__(torch._C.PyTorchFileWriter(str(name)))
RuntimeError: Parent directory ../outputs/models does not exist.
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1694, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1185, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1932, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 591, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 856, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 659, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 328, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 112, in <module>
    out = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 53, in forward
    src = self.embedding(src)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/utils.py", line 40, in forward
    return x + self.pos_encoding[:x.size(-2), :]
RuntimeError: The size of tensor a (10733) must match the size of tensor b (10000) at non-singleton dimension 1
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 591, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 857, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1933, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1185, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1694, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 329, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 476, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1074, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 941, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 659, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 8, in <module>
    from ignite.metrics import Accuracy
ModuleNotFoundError: No module named 'ignite.metrics'
Traceback (most recent call last):
  File "train.py", line 8, in <module>
    from ignite.metrics import Accuracy
ModuleNotFoundError: No module named 'ignite.metrics'
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Sigmoid: 1-5                                     [1, 200, 1]               --
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 85, in <module>
    val_loss += criterion(out[:,int(x.size(-2)*TRAIN_INTERVAL[1]):,:], y[:,int(x.size(-2)*TRAIN_INTERVAL[1]),:])
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 619, in forward
    return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 3089, in binary_cross_entropy
    raise ValueError(
ValueError: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1186, 1])) is deprecated. Please ensure they have the same size.
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingStandard: 2-2             [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingStandard: 2-4             [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─MyIdentity: 2-5                             [1, 200, 64]              --
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        41,984
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Sigmoid: 1-5                                     [1, 200, 1]               --
====================================================================================================
Total params: 44,545
Trainable params: 44,545
Non-trainable params: 0
Total mult-adds (M): 0.01
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.87
Params size (MB): 0.05
Estimated Total Size (MB): 0.92
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 127, in <module>
    test_loss += criterion(out[:,int(x.size(-2)*VAL_INTERVAL[1]):,:], y[:,int(x.size(-2)*VAL_INTERVAL[1]),:])
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 619, in forward
    return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 3089, in binary_cross_entropy
    raise ValueError(
ValueError: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 659, 1])) is deprecated. Please ensure they have the same size.
Traceback (most recent call last):
  File "train.py", line 53, in <module>
    model = Transformer(N_FEATURES, N_EMBEDDING, N_HEADS, N_ENC_LAYERS, N_DEC_LAYERS, N_FORWARD)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 20, in __init__
    PositionalEncodingNLP(d_model))
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/utils.py", line 33, in __init__
    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
NameError: name 'math' is not defined
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingNLP: 2-2                  [1, 200, 64]              --
│    │    └─Dropout: 3-1                           [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingNLP: 2-4                  [1, 200, 64]              --
│    │    └─Dropout: 3-2                           [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─TransformerEncoder: 2-5                     [1, 200, 64]              --
│    │    └─ModuleList: 3-3                        --                        50,432
│    │    └─LayerNorm: 3-4                         [1, 200, 64]              128
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-5                        --                        83,968
│    │    └─LayerNorm: 3-6                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Sigmoid: 1-5                                     [1, 200, 1]               --
====================================================================================================
Total params: 137,089
Trainable params: 137,089
Non-trainable params: 0
Total mult-adds (M): 0.02
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 1.49
Params size (MB): 0.08
Estimated Total Size (MB): 1.57
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 72, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 54, in forward
    src = self.embedding(src)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/utils.py", line 41, in forward
    x = x + self.pe[:x.size(-2), :]
RuntimeError: The size of tensor a (8590) must match the size of tensor b (5000) at non-singleton dimension 1
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingNLP: 2-2                  [1, 200, 64]              --
│    │    └─Dropout: 3-1                           [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingNLP: 2-4                  [1, 200, 64]              --
│    │    └─Dropout: 3-2                           [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─TransformerEncoder: 2-5                     [1, 200, 64]              --
│    │    └─ModuleList: 3-3                        --                        50,432
│    │    └─LayerNorm: 3-4                         [1, 200, 64]              128
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-5                        --                        83,968
│    │    └─LayerNorm: 3-6                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Sigmoid: 1-5                                     [1, 200, 1]               --
====================================================================================================
Total params: 137,089
Trainable params: 137,089
Non-trainable params: 0
Total mult-adds (M): 0.02
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 1.49
Params size (MB): 0.08
Estimated Total Size (MB): 1.57
====================================================================================================
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingNLP: 2-2                  [1, 200, 64]              --
│    │    └─Dropout: 3-1                           [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingNLP: 2-4                  [1, 200, 64]              --
│    │    └─Dropout: 3-2                           [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─TransformerEncoder: 2-5                     [1, 200, 64]              --
│    │    └─ModuleList: 3-3                        --                        50,432
│    │    └─LayerNorm: 3-4                         [1, 200, 64]              128
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-5                        --                        83,968
│    │    └─LayerNorm: 3-6                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Identity: 1-5                                    [1, 200, 1]               --
====================================================================================================
Total params: 137,089
Trainable params: 137,089
Non-trainable params: 0
Total mult-adds (M): 0.02
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 1.49
Params size (MB): 0.08
Estimated Total Size (MB): 1.57
====================================================================================================
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingNLP: 2-2                  [1, 200, 64]              --
│    │    └─Dropout: 3-1                           [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingNLP: 2-4                  [1, 200, 64]              --
│    │    └─Dropout: 3-2                           [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─TransformerEncoder: 2-5                     [1, 200, 64]              --
│    │    └─ModuleList: 3-3                        --                        50,432
│    │    └─LayerNorm: 3-4                         [1, 200, 64]              128
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-5                        --                        83,968
│    │    └─LayerNorm: 3-6                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Identity: 1-5                                    [1, 200, 1]               --
====================================================================================================
Total params: 137,089
Trainable params: 137,089
Non-trainable params: 0
Total mult-adds (M): 0.02
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 1.49
Params size (MB): 0.08
Estimated Total Size (MB): 1.57
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 75, in <module>
    loss.backward()
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 39.59 GiB total capacity; 771.41 MiB already allocated; 94.19 MiB free; 798.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingNLP: 2-2                  [1, 200, 64]              --
│    │    └─Dropout: 3-1                           [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingNLP: 2-4                  [1, 200, 64]              --
│    │    └─Dropout: 3-2                           [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─TransformerEncoder: 2-5                     [1, 200, 64]              --
│    │    └─ModuleList: 3-3                        --                        50,432
│    │    └─LayerNorm: 3-4                         [1, 200, 64]              128
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-5                        --                        83,968
│    │    └─LayerNorm: 3-6                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Identity: 1-5                                    [1, 200, 1]               --
====================================================================================================
Total params: 137,089
Trainable params: 137,089
Non-trainable params: 0
Total mult-adds (M): 0.02
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 1.49
Params size (MB): 0.08
Estimated Total Size (MB): 1.57
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 75, in <module>
    loss.backward()
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 39.59 GiB total capacity; 771.41 MiB already allocated; 94.19 MiB free; 798.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingLearned: 2-2              [1, 200, 64]              --
│    │    └─Sequential: 3-1                        [15000, 64]               8,320
│    │    └─Dropout: 3-2                           [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingLearned: 2-4              [1, 200, 64]              (recursive)
│    │    └─Sequential: 3-3                        [15000, 64]               (recursive)
│    │    └─Dropout: 3-4                           [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─TransformerEncoder: 2-5                     [1, 200, 64]              --
│    │    └─ModuleList: 3-5                        --                        50,432
│    │    └─LayerNorm: 3-6                         [1, 200, 64]              128
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-7                        --                        83,968
│    │    └─LayerNorm: 3-8                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Identity: 1-5                                    [1, 200, 1]               --
====================================================================================================
Total params: 145,409
Trainable params: 145,409
Non-trainable params: 0
Total mult-adds (M): 249.62
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 32.21
Params size (MB): 0.11
Estimated Total Size (MB): 32.33
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 76, in <module>
    loss.backward()
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 39.59 GiB total capacity; 786.46 MiB already allocated; 74.19 MiB free; 818.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Sequential: 1-1                                  [1, 200, 64]              --
│    └─Linear: 2-1                                 [1, 200, 64]              320
│    └─PositionalEncodingLearned: 2-2              [1, 200, 64]              --
│    │    └─Sequential: 3-1                        [15000, 64]               8,320
│    │    └─Dropout: 3-2                           [1, 200, 64]              --
├─Sequential: 1-2                                  [1, 200, 64]              (recursive)
│    └─Linear: 2-3                                 [1, 200, 64]              (recursive)
│    └─PositionalEncodingLearned: 2-4              [1, 200, 64]              (recursive)
│    │    └─Sequential: 3-3                        [15000, 64]               (recursive)
│    │    └─Dropout: 3-4                           [1, 200, 64]              --
├─Transformer: 1-3                                 [1, 200, 64]              --
│    └─TransformerEncoder: 2-5                     [1, 200, 64]              --
│    │    └─ModuleList: 3-5                        --                        50,432
│    │    └─LayerNorm: 3-6                         [1, 200, 64]              128
│    └─TransformerDecoder: 2-6                     [1, 200, 64]              --
│    │    └─ModuleList: 3-7                        --                        83,968
│    │    └─LayerNorm: 3-8                         [1, 200, 64]              128
├─Sequential: 1-4                                  [1, 200, 1]               --
│    └─Linear: 2-7                                 [1, 200, 32]              2,080
│    └─Dropout: 2-8                                [1, 200, 32]              --
│    └─ReLU: 2-9                                   [1, 200, 32]              --
│    └─Linear: 2-10                                [1, 200, 1]               33
├─Identity: 1-5                                    [1, 200, 1]               --
====================================================================================================
Total params: 145,409
Trainable params: 145,409
Non-trainable params: 0
Total mult-adds (M): 249.62
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 32.21
Params size (MB): 0.11
Estimated Total Size (MB): 32.33
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 76, in <module>
    loss.backward()
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 0; 39.59 GiB total capacity; 786.46 MiB already allocated; 74.19 MiB free; 818.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torchinfo/torchinfo.py", line 295, in forward_pass
    _ = model(*x, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 61, in forward
    src = self.positional_encoding(src, t=t)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
TypeError: forward() got an unexpected keyword argument 't'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "train.py", line 64, in <module>
    model_info = summary(model, input_size=[(BATCH_SIZE, 200, N_FEATURES), (BATCH_SIZE, 200, N_FEATURES)])
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torchinfo/torchinfo.py", line 223, in summary
    summary_list = forward_pass(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torchinfo/torchinfo.py", line 304, in forward_pass
    raise RuntimeError(
RuntimeError: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Linear: 1]
Traceback (most recent call last):
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torchinfo/torchinfo.py", line 295, in forward_pass
    _ = model(*x, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 61, in forward
    src = self.positional_encoding(src)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/utils.py", line 41, in forward
    x = x + self.pe[:x.size(-2), :]
RuntimeError: The size of tensor a (64) must match the size of tensor b (4) at non-singleton dimension 2

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "train.py", line 64, in <module>
    model_info = summary(model, input_size=[(BATCH_SIZE, 200, N_FEATURES), (BATCH_SIZE, 200, N_FEATURES)])
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torchinfo/torchinfo.py", line 223, in summary
    summary_list = forward_pass(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torchinfo/torchinfo.py", line 304, in forward_pass
    raise RuntimeError(
RuntimeError: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Linear: 1]
Traceback (most recent call last):
  File "train.py", line 62, in <module>
    model = Transformer(N_FEATURES, N_EMBEDDING, N_HEADS, N_ENC_LAYERS, N_DEC_LAYERS, N_FORWARD, binary=BINARY)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 21, in __init__
    self.positional_encoding = PositionalEncodingNLP(self.d_model, dropout=dropout)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'Transformer' object has no attribute 'd_model'
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              320
├─PositionalEncodingNLP: 1-2                       [1, 200, 64]              --
│    └─Dropout: 2-1                                [1, 200, 64]              --
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─PositionalEncodingNLP: 1-4                       [1, 200, 64]              --
│    └─Dropout: 2-2                                [1, 200, 64]              --
├─Transformer: 1-5                                 [1, 200, 64]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 64]              --
│    │    └─ModuleList: 3-1                        --                        50,432
│    │    └─LayerNorm: 3-2                         [1, 200, 64]              128
│    └─TransformerDecoder: 2-4                     [1, 200, 64]              --
│    │    └─ModuleList: 3-3                        --                        83,968
│    │    └─LayerNorm: 3-4                         [1, 200, 64]              128
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 32]              2,080
│    └─Dropout: 2-6                                [1, 200, 32]              --
│    └─ReLU: 2-7                                   [1, 200, 32]              --
│    └─Linear: 2-8                                 [1, 200, 1]               33
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 137,089
Trainable params: 137,089
Non-trainable params: 0
Total mult-adds (M): 0.02
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 1.49
Params size (MB): 0.08
Estimated Total Size (MB): 1.57
====================================================================================================
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              320
├─Time2Vec: 1-2                                    [1, 200, 68]              --
│    └─Linear: 2-1                                 [200, 4]                  8
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 68]              (recursive)
│    └─Linear: 2-2                                 [200, 4]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 68]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 68]              --
│    │    └─ModuleList: 3-1                        --                        139,380
│    │    └─LayerNorm: 3-2                         [1, 200, 68]              136
│    └─TransformerDecoder: 2-4                     [1, 200, 68]              --
│    │    └─ModuleList: 3-3                        --                        233,900
│    │    └─LayerNorm: 3-4                         [1, 200, 68]              136
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 34]              2,346
│    └─Dropout: 2-6                                [1, 200, 34]              --
│    └─ReLU: 2-7                                   [1, 200, 34]              --
│    └─Linear: 2-8                                 [1, 200, 1]               35
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 376,261
Trainable params: 376,261
Non-trainable params: 0
Total mult-adds (M): 0.05
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 3.18
Params size (MB): 0.20
Estimated Total Size (MB): 3.38
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 86, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 65, in forward
    output = self.transformer(src, tgt, tgt_mask=self.tgt_mask, src_mask=self.src_mask, memory_mask=self.memory_mask)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 146, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 369, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 717, in forward
    x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 735, in _mha_block
    x = self.multihead_attn(x, mem, mem,
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1205, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/functional.py", line 5373, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.59 GiB total capacity; 8.69 GiB already allocated; 285.19 MiB free; 8.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              320
├─Time2Vec: 1-2                                    [1, 200, 68]              --
│    └─Linear: 2-1                                 [200, 4]                  8
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 68]              (recursive)
│    └─Linear: 2-2                                 [200, 4]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 68]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 68]              --
│    │    └─ModuleList: 3-1                        --                        139,380
│    │    └─LayerNorm: 3-2                         [1, 200, 68]              136
│    └─TransformerDecoder: 2-4                     [1, 200, 68]              --
│    │    └─ModuleList: 3-3                        --                        233,900
│    │    └─LayerNorm: 3-4                         [1, 200, 68]              136
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 34]              2,346
│    └─Dropout: 2-6                                [1, 200, 34]              --
│    └─ReLU: 2-7                                   [1, 200, 34]              --
│    └─Linear: 2-8                                 [1, 200, 1]               35
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 376,261
Trainable params: 376,261
Non-trainable params: 0
Total mult-adds (M): 0.05
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 3.18
Params size (MB): 0.20
Estimated Total Size (MB): 3.38
====================================================================================================
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              320
├─Time2Vec: 1-2                                    [1, 200, 68]              --
│    └─Linear: 2-1                                 [200, 4]                  8
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 68]              (recursive)
│    └─Linear: 2-2                                 [200, 4]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 68]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 68]              --
│    │    └─ModuleList: 3-1                        --                        55,752
│    │    └─LayerNorm: 3-2                         [1, 200, 68]              136
│    └─TransformerDecoder: 2-4                     [1, 200, 68]              --
│    │    └─ModuleList: 3-3                        --                        93,560
│    │    └─LayerNorm: 3-4                         [1, 200, 68]              136
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 34]              2,346
│    └─Dropout: 2-6                                [1, 200, 34]              --
│    └─ReLU: 2-7                                   [1, 200, 34]              --
│    └─Linear: 2-8                                 [1, 200, 1]               35
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 152,293
Trainable params: 152,293
Non-trainable params: 0
Total mult-adds (M): 0.02
====================================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 1.57
Params size (MB): 0.09
Estimated Total Size (MB): 1.66
====================================================================================================
Traceback (most recent call last):
  File "train.py", line 86, in <module>
    outputs = model(x, x, enc_window=ENC_WINDOW, dec_window=DEC_WINDOW, mem_window=MEM_WINDOW)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/test/market_prediction/src/transformer.py", line 60, in forward
    src = self.embedding(src)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (6139x36 and 4x64)
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._transformer_encoder_layer_fwd(
/home/ansysai/lkloker/transformersPDE/torch_leon/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)
  return torch._native_multi_head_attention(
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
Transformer                                        [1, 200, 1]               --
├─Linear: 1-1                                      [1, 200, 64]              2,368
├─Time2Vec: 1-2                                    [1, 200, 68]              --
│    └─Linear: 2-1                                 [200, 4]                  8
├─Linear: 1-3                                      [1, 200, 64]              (recursive)
├─Time2Vec: 1-4                                    [1, 200, 68]              (recursive)
│    └─Linear: 2-2                                 [200, 4]                  (recursive)
├─Transformer: 1-5                                 [1, 200, 68]              --
│    └─TransformerEncoder: 2-3                     [1, 200, 68]              --
│    │    └─ModuleList: 3-1                        --                        55,752
│    │    └─LayerNorm: 3-2                         [1, 200, 68]              136
│    └─TransformerDecoder: 2-4                     [1, 200, 68]              --
│    │    └─ModuleList: 3-3                        --                        93,560
│    │    └─LayerNorm: 3-4                         [1, 200, 68]              136
├─Sequential: 1-6                                  [1, 200, 1]               --
│    └─Linear: 2-5                                 [1, 200, 34]              2,346
│    └─Dropout: 2-6                                [1, 200, 34]              --
│    └─ReLU: 2-7                                   [1, 200, 34]              --
│    └─Linear: 2-8                                 [1, 200, 1]               35
├─Identity: 1-7                                    [1, 200, 1]               --
====================================================================================================
Total params: 154,341
Trainable params: 154,341
Non-trainable params: 0
Total mult-adds (M): 0.03
====================================================================================================
Input size (MB): 0.06
Forward/backward pass size (MB): 1.57
Params size (MB): 0.09
Estimated Total Size (MB): 1.72
====================================================================================================
